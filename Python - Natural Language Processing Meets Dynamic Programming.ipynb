{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python: Natural Language Processing Meets Dynamic Programming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational expense is an incredibly important consideration in the fields of machine learning. Natural Language Processing is no exception. After all, what good are the models and algorithms we build if they're unable to scale or require the use of GPUs? \n",
    "\n",
    "This often leads to creative thinking to reduce the computational power required by different algorithms. In this tutorial, we'll explore how dynamic programming is often used as a technique to accomplish just that. We'll explore the rhelm of natural language processing, specific exploring the Viterbi Algorithm, often used in word tagging. \n",
    "\n",
    "If you're unfamiliar with Dynamic Programming, I recommend you take a look at this resource. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup \n",
    "\n",
    "To begin with this tutorial, make sure to download the dataset we'll be using from [this]() GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "I briefly mentioned the Viterbi Algorithm as a dynamical programming algorithm used in Natural Language Processing. This is particularly powerful because it allows us to compute the most probable path. Its importance will be show in this tutorial, as we review its application with word tagging. This Viterbi algorithm will compute the most probable series of word tags on a given sentence.\n",
    "\n",
    "You might ask yourself, \"what makes this algorithm so special?\" Because this algorithm allowz us to utilize dynamic programming, we avoid the issue of having to use brute force. For example, if we search and compare all possible sequences, we can find a result. That, however, is highly inefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with many programming projects, we'll require the use of helper functions for our Viterbi Algorithm. Specifically, we'll need `pi` and `theta` data structures represent our the highest log probabilities for each position and the previous maximum log probabilities of the previous words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you'll notice two very similar functions, `get_pi` and `get_theta`. If you're unfamiliar with default dictionaries, they're simply data structures which allow you to avoid key errors in times where a dictionary key value might not exist. This might not be completely clear as to _why_ this might occur, but we'll soon enough get into the details of that. \n",
    "\n",
    "This data structure is confusing, so let's break it down. `pi` essentially has three levels for which you can index the structure. For example, `pi[1][x][y]` would indicate the highest log probability in position 0 with tag x _at_ position 0 and tag y preceding x. Since the position of y would be negative one in this case, it would be before the sentence starts, which is why we consider start and stop symbols in this algorithm. This is why we've purposely set the length of _both_ lists to the number of words added to 2.\n",
    "\n",
    "Following that similar line of thought, `theta` is _also_ three levels for which we can index the structure. `theta[1][x][y]` would return the tag previous to yx that led to the maximum log probabilities in `pi`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pi(words):\n",
    "    return [defaultdict(dict) for x in range(len(words)+2)]\n",
    "\n",
    "def get_theta(words):\n",
    "    return [defaultdict(dict) for x in range(len(words)+2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get started! We'll begin with with reviewing the inputs and _why_ these are important to our algorithm. In the following function header, there are five inputs. `sentences` will a list where each list corresponds to a string of word tags. \n",
    "\n",
    "Next, there's `words_seen` which serves as the vocabulary. For the purpose of simplicity, we'll assume a known set of tags, which is our third parameter `tag_list`. Then there's `q_values` which will be a dictionary with the trigram probabilities. Again, we'll assume those values are given to us for simplicity. \n",
    "\n",
    "On that same line of reasoning, we'll assume emission values are calculated and do _not_ change over time, which will be input as a dictionary in `e_values`. For those unfamiliar with emission probabilities, an emission probability is the probability of an observation being generated from a state. In this case, that state is the two previous word tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentences, words_seen, tag_list, q_values, e_values):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is ultimately going to write to a file where each line is the sequence of predicted tags. We'll call this file `5_2.txt` and open it in writing mode for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentences, words_seen, tag_list, e_values, q_values):\n",
    "    w1 = open('5_2.txt', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're onto the important part of iterating through each sentence. \n",
    "\n",
    "Notes:\n",
    "- mention why we need tagged list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentences, words_seen, tag_list, q_values, e_values):\n",
    "    w1 = open('5_2.txt', 'w')\n",
    "    tagged = []\n",
    "    # first step is to iterate through the sentences \n",
    "    for words in sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a processing step, we'll need to split each sentence into a list of its words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def viterbi(sentences, words_seen, tag_list, q_values, e_values):\n",
    "    w1 = open('5_2.txt', 'w')\n",
    "    tagged = []\n",
    "    # first step is to iterate through the sentences \n",
    "    for words in sentences:\n",
    "        \n",
    "        # transform into list of words\n",
    "        words = words.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the pi data structure with a function call so that we can actually store the highest log probability tags for each position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for words in sentences:\n",
    "        \n",
    "        # transform into list of words\n",
    "        words = words.split()\n",
    "\n",
    "        # each entry will represent the highest log-probability tags for each position\n",
    "        pi = get_pi(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to start off with populating `pi`, we initialize the first case `pi[0]['*']['*']` to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # first step is to iterate through the sentences \n",
    "    for words in sentences:\n",
    "        \n",
    "        # transform into list of words\n",
    "        words = words.split()\n",
    "\n",
    "        # each entry will represent the highest log-probability tags for each position\n",
    "        pi = get_pi(words)\n",
    "\n",
    "        # base case\n",
    "        pi[0]['*']['*'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence, we'll need to iterate through all its words to calculate the probabilities and store its tags. This leads to the nested for loop seen below. In this second for loop, first the word stored to a variable for easy reference. After that, to avoid errors regarding words in the test set that don't show up in training, we check for this first and handle it with the `_RARE_` symbol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # each entry will represent the highest log-probability tags for each position\n",
    "        pi = get_pi(words)\n",
    "\n",
    "        # base case\n",
    "        pi[0]['*']['*'] = 0\n",
    "\n",
    "        # each entry represents the tag right before that led to the max log-prob\n",
    "        theta = get_theta(words)\n",
    "\n",
    "        for i in range(0, len(words)):\n",
    "            word = words[i]\n",
    "\n",
    "            # replace with _RARE_ if it is a rare word\n",
    "            if word not in words_seen:\n",
    "                word = \"_RARE_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that rare words are taken care of, iterating through the possible tags is critical for extracting the emission log probabilty of the current word from the inputted `e_values` since we need to actually find the path that has the highest probability generating the current word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # base case\n",
    "        pi[0]['*']['*'] = 0\n",
    "\n",
    "        # each entry represents the tag right before that led to the max log-prob\n",
    "        theta = get_theta(words)\n",
    "\n",
    "        for i in range(0, len(words)):\n",
    "            word = words[i]\n",
    "\n",
    "            # replace with _RARE_ if it is a rare word\n",
    "            if word not in words_seen:\n",
    "                word = \"_RARE_\"\n",
    "\n",
    "            # ~~~ NEW CODE BELOW ~~\n",
    "\n",
    "            # for each possible tag or the current word \n",
    "            for t in tag_list:\n",
    "\n",
    "                emission_log_prob = e_values[(word,t)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the maximum probability, we can now _backtrack_ to previous tags to find the correct path by iterating through all the `pi` keys.. Not all tags have a probability; in fact, some might not have any, so we have to iterate through the `pi` keys that actually exist with a probability. In the cases where there is so  possible trigram, we consider log of infinity to speed up the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for i in range(0, len(words)):\n",
    "            word = words[i]\n",
    "\n",
    "            # replace with _RARE_ if it is a rare word\n",
    "            if word not in words_seen:\n",
    "                word = \"_RARE_\"\n",
    "\n",
    "            # for each possible tag for the current word (the rightmost word of the trigram)\n",
    "            for t in tag_list:\n",
    "\n",
    "                emission_log_prob = e_values[(word,t)]\n",
    "\n",
    "                # ~~~ NEW CODE BELOW ~~\n",
    "                # backtrack to previous tags \n",
    "                for m in pi[i].keys():\n",
    "                    # first we only look at the trigrams with a probability,\n",
    "                    for prob0_tri in [False, True]:\n",
    "                        # checks all possible initial tags for the trigram, but stores the best one\n",
    "                        best_tri_tag = None\n",
    "                        best_log_prob = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to transitions that aren't even possible, we can simply skip this computation, which is why the `else` statement in this portion of the code contains the key word `continue`. In the cases where the tag just doesn't exist, however, we once again set the value to a log of infinity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # backtrack to possible previous tags \n",
    "                for m in pi[i].keys():\n",
    "                    # first we only look at the trigrams with a probability, otherwise we use the log prob of 0\n",
    "                    for prob0_tri in [False, True]:\n",
    "                        # checks all possible initial tags for the trigram, but stores the best one\n",
    "                        best_tri_tag = None\n",
    "                        best_log_prob = None\n",
    "\n",
    "                        # ~~~ NEW CODE BELOW ~~~\n",
    "\n",
    "                        for left_tri_tag, old in pi[i][m].items():\n",
    "                            if (left_tri_tag, m, t) not in q_values:\n",
    "                                if prob0_tri:\n",
    "                                    q_values[(left_tri_tag, m, t)] = math.inf\n",
    "                                else:\n",
    "                                    continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the purpose of this tutorial is to emphasize speed. In the spirit of that, we can ignore emission for now since it is constant for all left trigram tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        for left_tri_tag, old in pi[i][m].items():\n",
    "                            if (left_tri_tag, m, t) not in q_values:\n",
    "                                if prob0_tri:\n",
    "                                    q_values[(left_tri_tag, m, t)] = math.inf\n",
    "                                else:\n",
    "                                    continue\n",
    "\n",
    "                            # ~~~ NEW CODE BELOW ~~~\n",
    "\n",
    "                            log_prob = old + q_values[(left_tri_tag, m, t)]\n",
    "                            if not best_log_prob or log_prob >= best_log_prob:\n",
    "                                best_log_prob = log_prob\n",
    "                                best_tri_tag = left_tri_tag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the STOP probability by going through all possible two last tags of the sentence and chosing the combination that maximizes probability when we transition to STOP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # compute STOP probability\n",
    "        # Let's go through all possible two last tags of the sentence and chose the combination that maximizes probability when we transition to STOP\n",
    "        for m in pi[-2].keys():\n",
    "            # first we only look at the trigrams with some probability, if no option is found we fall back to considering -1000 as log probability of \"impossible\" trigrams\n",
    "            for prob0_tri in [False, True]:\n",
    "                # lets check all possible previous to previous tags (historic prob is non-zero), and store the best one\n",
    "                best_tri_tag = None\n",
    "                best_log_prob = None\n",
    "                for left_tri_tag, old in pi[-2][m].items():\n",
    "                    # if the transition is impossible don't even bother, unless we are already in second iteration because we found no other option\n",
    "                    if (left_tri_tag, m, 'STOP') not in q_values:\n",
    "                        if prob0_tri:\n",
    "                            q_values[(left_tri_tag, m, 'STOP')] = math.inf\n",
    "                        else:\n",
    "                            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next we ccompute the probability of the transition and previous probability -- this is why this addition is occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    # compute the probability of the transition + historic probability\n",
    "                    log_prob = old + q_values[(left_tri_tag, m, 'STOP')]\n",
    "                    if not best_log_prob or log_prob >= best_log_prob:\n",
    "                        best_log_prob = log_prob\n",
    "                        best_tri_tag = left_tri_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we reach the end of computing the most probable sequence of tags. Our final step is to add an emission probability of 0 to the end. Since the STOP symbol will _always_ be at the end, it will _always_ have an emission probability of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                # STOP has emission prob of 100%, so just save\n",
    "                if best_tri_tag:\n",
    "                    pi[-1]['STOP'][m] = best_log_prob\n",
    "                    theta[-1]['*'][m] = best_tri_tag\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we've found the most probable path, we obviously want to output it. For this tutorial, I've decide to write the output to a file. But before we even get to that part, we have to backtrack once again to find the sequence. This process begins with finding the last tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # now backtrack to find out sequence\n",
    "        # Start by finding last tag (not counting STOP)\n",
    "        last_tag = None\n",
    "        best_sent_prob = None\n",
    "        for tag, sent_log_prob in pi[-1]['STOP'].items():\n",
    "            if not best_sent_prob or sent_log_prob > best_sent_prob:\n",
    "                best_sent_prob = sent_log_prob\n",
    "                last_tag = tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, the previous step involved backtracking for the STOP symbol. Now we have to continue on for the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        sentence_tags = deque([last_tag, 'STOP'])\n",
    "        for i in range(0,len(words)-1):\n",
    "            # discover the tag that lead to the next two\n",
    "            left_tri_tag = theta[-1-i][sentence_tags[1]][sentence_tags[0]]\n",
    "            # add it to tags\n",
    "            sentence_tags.appendleft(left_tri_tag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we have the tags recovered, we can go ahead and write to the file after putting them into a cleaner format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # build the string with words and tags\n",
    "        sentence = []\n",
    "        for i in range(len(words)):\n",
    "            sentence.append(words[i] + \"/\" + sentence_tags[i])\n",
    "\n",
    "        w1.write(\" \".join(sentence) + \"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's put it all together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
